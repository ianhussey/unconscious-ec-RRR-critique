---
title: "Evaluative Conditioning without awareness: Replicable effects do not equate replicable inferences"
subtitle: "Analyses"
author: "Ian Hussey^[Ghent University. Email: ian.hussey@ugent.be]"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)

# options
options(knitr.table.format = "html") # necessary configuration of tables

# disable scientific notation
options(scipen = 999) 

```

# Data, dependencies & functions

```{r}

# Dependencies ----

check_for_and_install_packages <- function(package) {
  if (!package %in% installed.packages()) install.packages(package)
}

check_for_and_install_packages("tidyverse")
check_for_and_install_packages("metafor")
check_for_and_install_packages("knitr")
check_for_and_install_packages("kableExtra")
check_for_and_install_packages("BayesFactor")
check_for_and_install_packages("broom")
if (!"patchwork" %in% installed.packages()) devtools::install_github("thomasp85/patchwork")

library(tidyverse)
library(metafor)
library(knitr)
library(kableExtra)
library(BayesFactor)
library(broom)
library(patchwork)

# print versions of packages used, R version, OS details.
writeLines(capture.output(sessionInfo()), "session_info.txt")


# Data ----

data_processed <- read.csv("../../unconscious-ec-RRR/data/processed/data_processed.csv") %>%
  mutate(data_collection_site = dplyr::recode(data_collection_site,
                                              "Balas and Sarzynnska" = "Balas",
                                              "Corneille and Mierop" = "Mierop",
                                              "Gast Richter and Benedict" = "Gast",
                                              "Gawronski" = "Gawronski",
                                              "Hutter" = "HÃ¼tter",
                                              "Kurdi and Ferguson" = "Kurdi",
                                              "Moran Hussey and Hughes" = "Moran",
                                              "Olsen and Fritzlen" = "Olson",
                                              "Smith and Douglas" = "Douglas",
                                              "Stahl Bading Aust Heycke and Thomasius" = "Stahl",
                                              "Unkelbach and Hogden" = "Unkelbach",
                                              "Vadillo" = "Vadillo")) %>%
  filter(exclude_surveillance == FALSE & 
           simulated_data == FALSE) %>%
  mutate(exclude_all_four_combined = ifelse(exclude_aware_olsen_and_fazio +
                                              exclude_aware_olsen_and_fazio_modified +
                                              exclude_awareness_baranan_dehouwer_nosek +
                                              exclude_awareness_baranan_dehouwer_nosek_modified > 0, 1, 0)) %>%
  rename(DV = sum_score_evaluation_CSpos_preferred) %>%
  mutate(DV_uninverted = ifelse(condition == "CS1_USneg", DV*-1,
                                ifelse(condition == "CS1_USpos", DV, NA))) %>%
  dplyr::select(data_collection_site,
                DV,
                DV_uninverted,
                condition,
                exclude_aware_olsen_and_fazio,
                exclude_aware_olsen_and_fazio_modified,
                exclude_awareness_baranan_dehouwer_nosek,
                exclude_awareness_baranan_dehouwer_nosek_modified,
                exclude_all_four_combined)
  

data_combined_criteria <- data_processed %>%
  filter(exclude_all_four_combined == FALSE)


# Define functions ----

# helper function to calculate G scores for each bootstrap resample
bootstrap_helper_function <- function(split, ...) {
  
  require(dplyr)
  
  results <- analysis(split) %>%
    gather(item, response, c(-id)) %>%
    arrange(id) %>%
    group_by(id) %>%
    mutate(guttman_error = ifelse(dplyr::lag(response) < response, 1, 0)) %>%
    dplyr::summarize(guttman_error_boolean = max(guttman_error, na.rm = TRUE)) %>%
    ungroup() %>%
    dplyr::summarize(G = sum(guttman_error_boolean, na.rm = TRUE)/n()) %>%
    mutate(`G*` = G/(ncol(analysis(split)) - 1)) %>% # normalized by the number of variables
    gather(metric, estimate)
  
  return(results)
  
}


# function to bootstrap G values
bootstrap_guttman_errors <- function(data, n.iter = 2000, ...){
  
  require(dplyr)
  require(purrr)
  require(rsample)
  
  # create bootstraps
  bootstraps <- data %>%
    rownames_to_column(var = "id") %>%
    bootstraps(times = n.iter)
  
  ## apply to each bootstrap, then summarize across them
  results_bootstrapped_guttman_errors <- bootstraps %>% 
    mutate(bootstrapped_results = map(splits, bootstrap_helper_function)) %>% 
    unnest(bootstrapped_results) %>% 
    group_by(metric) %>%
    dplyr::summarize(median = quantile(estimate, 0.500),
                     ci_lwr = quantile(estimate, 0.025),
                     ci_upr = quantile(estimate, 0.975))
  
  return(results_bootstrapped_guttman_errors)
  
}

# add heterogeneity metrics to metafor forest plot
add_heterogeneity_metrics_to_forest <- function(fit) {
  bquote(paste("RE Model (", 
               italic('I')^"2", " = ", .(formatC(format(round(fit$I2, 1), nsmall = 1))),
               "%, ", italic('H')^"2", " = ", .(formatC(format(round(fit$H2, 1), nsmall = 1))), ")"))
}

# function to round all numerics in a data frame
round_df <- function(df, digits) {
  nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
  df[,nums] <- round(df[,nums], digits = digits)
  (df)
}

# apa format p value
apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", round(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

# function to meta analyze proportions
meta_analysis_proportions <- function(data){
  
  meta_data_1 <- escalc(measure = "PR", 
                        xi = criterion, 
                        ni = ni, 
                        data = data)
  
  tmp <- t(sapply(split(meta_data_1, meta_data_1$data_collection_site), 
                  function(x) binom.test(x$criterion, x$ni)$conf.int))
  meta_data_1$ci.lb <- tmp[,1]
  meta_data_1$ci.ub <- tmp[,2]
  
  fit <- rma.glmm(measure = "PLO", 
                  xi = criterion, 
                  ni = ni, 
                  data = data, 
                  slab = data_collection_site)
  
  return(fit)
  
}


#' A priori power analysis for meta analysis  -----

#' Estimation power for meta analysis of effects, using equations derived from Valentine, Pigott, & Rothstein (2010, doi: 10.3102/1076998609346961), derived by Quintana (2017: https://towardsdatascience.com/how-to-calculate-statistical-power-for-your-meta-analysis-e108ee586ae8)
#' @param yi Meta-analyzed Cohen's d effect size. 
#' @param ni The average number of data points per site
#' @param k The number of sites
#' @param tau2 The tau^2 metric of between site heterogeneity
#' @return Statistical power (i.e., 1 - Beta): the probability of observing a significant result given the parameters. 

power_meta <- function(yi, ni, k, tau2){
  
  eq1 <- ((ni + ni)/((ni)*(ni))) + ((yi^2)/(2*(ni + ni)))
  eq2 <- tau2*(eq1)
  eq3 <- eq2+eq1
  eq4 <- eq3/k
  eq5 <- (yi/sqrt(eq4))
  power <- (1 - pnorm(1.96 - eq5)) # two-tailed
  
  return(power)
}


# meta analysis and forest plot workflow
meta_analysis_workflow <- function(data, 
                                   effect_size_label = "Hedges' g", 
                                   reference_line = 0, 
                                   plot = TRUE) {
  
  # calculate effect sizes for meta
  # NB the original preregistered code used a bootstrapping method to calculate effect sizes, CIs, and SEIs. However, the unexpectedly small number of participants collected at some sites meant that results - particularly heterogeneity effect sizes - were relatively unstable across re-running the script. For the sake of computational reproducibility, I therefore exchange the bootstrapping method for the arithmetic method throughout. Sites with N <= 2 are then excluded so that ES can be calculated (see [link](https://www.meta-analysis.com/downloads/Meta-analysis%20Effect%20sizes%20based%20on%20means.pdf)).
  data_effect_sizes <- data %>%
    group_by(data_collection_site) %>%
    dplyr::summarize(preference_mean = mean(DV),
                     preference_sd = sd(DV),
                     preference_n = n()) %>%
    # must have greater than N=2 per site to calculate SD etc
    filter(preference_n > 2) %>%
    # calculate h and its SE
    dplyr::mutate(preference_cohens_dz = preference_mean/preference_sd,
                  cohens_dz_V = ((preference_n*2)/(preference_n^2)) +
                    ((preference_cohens_dz^2) / (preference_n*4)),
                  J = 1 - (3/(4*(preference_n-1)-1)),
                  hedges_g = preference_cohens_dz * J,
                  hedges_g_V = J^2 * cohens_dz_V,
                  hedges_g_se = sqrt(hedges_g_V)) %>%
    ungroup() %>%
    dplyr::select(data_collection_site, hedges_g, hedges_g_se)
  
  # fit random effects model 
  fitted_model <- 
    rma(yi   = hedges_g, 
        sei  = hedges_g_se,
        data = data_effect_sizes,
        slab = data_collection_site)
  
  p_value <- apa_p_value(fitted_model$pval)
  
  z_value <- fitted_model$zval
  
  # model predictions
  meta_analysis_results <-
    predict(fitted_model, digits = 5) %>%
    as.data.frame() %>%
    gather() %>%
    round_df(2) %>%
    dplyr::rename(metric = key,
                  estimate = value) %>%
    mutate(metric = dplyr::recode(metric,
                                  "pred"  = paste("Meta analysed ", effect_size_label),
                                  "ci.lb" = "95% CI lower",
                                  "ci.ub" = "95% CI upper",
                                  "cr.lb" = "95% CR lower",
                                  "cr.ub" = "95% CR upper"))
  
  meta_analysis_results <- rbind(meta_analysis_results,
                                 data.frame(metric = "p", estimate = p_value),
                                 data.frame(metric = "z", estimate = z_value))
  
  # summarize results
  meta_analysis_results_text <- 
    paste0("k = ", fitted_model$k, ", ", 
           effect_size_label, " = ", meta_analysis_results$estimate[1],
           # dynamic indexing of some values as different models return variables in different locations, but relative location is reliable
           ", 95% CI = [", meta_analysis_results$estimate[length(meta_analysis_results$estimate)-5],  
           ", ", meta_analysis_results$estimate[length(meta_analysis_results$estimate)-4], 
           "], 95% CR = [", meta_analysis_results$estimate[length(meta_analysis_results$estimate)-3], 
           ", ", meta_analysis_results$estimate[length(meta_analysis_results$estimate)-2],
           "], z = ", signif(as.numeric(as.character(meta_analysis_results$estimate[length(meta_analysis_results$estimate)])), digits = 3),
           ", p ", meta_analysis_results$estimate[length(meta_analysis_results$estimate)-1])
  
  heterogeneity_test_results_text <- 
    paste0("Q(df = ",    fitted_model$k - 1, ") = ", round(fitted_model$QE, 2), 
           ", p ",       apa_p_value(fitted_model$QEp),
           ", tau^2 = ", round(fitted_model$tau2, 2), 
           ", I^2 = ",   round(fitted_model$I2, 2),
           ", H^2 = ",   round(fitted_model$H2, 2))
  
  # forest plot 
  if (plot == TRUE) {
    forest_plot <- metafor::forest(fitted_model,
                                   xlab = effect_size_label,
                                   addcred = TRUE,
                                   refline = reference_line)
  } else {
    forest_plot <- NULL
  }
  
  return(list(data_effect_sizes               = data_effect_sizes,
              fitted_model                    = fitted_model, 
              meta_analysis_results           = meta_analysis_results,
              meta_analysis_results_text      = meta_analysis_results_text,
              heterogeneity_test_results_text = heterogeneity_test_results_text,
              plot                            = plot))
  
}

```

# Comparing awareness criteria 

Awareness exclusion rates after doing surveillance exclusions. 

## Rates by criterion 

```{r}

data_processed %>%
  select(exclude_aware_olsen_and_fazio, 
         exclude_aware_olsen_and_fazio_modified, 
         exclude_awareness_baranan_dehouwer_nosek, 
         exclude_awareness_baranan_dehouwer_nosek_modified,
         exclude_all_four_combined) %>%
  summarize_all(.funs = mean) %>%
  round_df(3) %>%
  gather() %>%
  arrange(value) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

## Incongruence between criteria

```{r}

results_incongruent_classification_rate <- data_processed %>%
  count(exclude_aware_olsen_and_fazio_modified,
        exclude_awareness_baranan_dehouwer_nosek_modified) %>%
  mutate(congruent = exclude_aware_olsen_and_fazio_modified == exclude_awareness_baranan_dehouwer_nosek_modified) %>%
  group_by(congruent) %>%
  dplyr::summarize(n = sum(n)) %>%
  ungroup() %>%
  dplyr::mutate(percent = round(n/(n+lead(n))*100, 1)) %>%
  pull(percent)

```

The criteria do not differ only in their strictness, but also the subsets of participants that they flag as aware. For example, `r results_incongruent_classification_rate[1]`% of participants receive incongruent awareness classifications between the O&F mod and BA,DH,N mod criteria - i.e., are classified by aware by one vs unaware by the other.

As such, the most severe test of the hypothesis that EC can occur in the absence of awareness is to apply all four criteria, and exclude participants who are flagged as aware by any of the four awareness tests. 

### Guttman errors

```{r}

# results run and saved to disk, as execution time is about 2 minutes

# NOTE THAT THE SELECT CALL MUST ORDER THE COLUMNS IN ASCENDING ORDER OF MEAN AWARENESS EXCLUSION RATE, AS TAKEN FROM THE ABOVE PRINTED TABLE, FOR THE ANALYSIS TO BE RUN APPROPRIATELY

# results_guttman_errors <- data_processed %>%
#   select(exclude_aware_olsen_and_fazio,
#          exclude_awareness_baranan_dehouwer_nosek_modified,
#          exclude_aware_olsen_and_fazio_modified,
#          exclude_awareness_baranan_dehouwer_nosek) %>%
#   bootstrap_guttman_errors()
#  
# save(results_guttman_errors, file = "models/results_guttman_errors.RData")
load("models/results_guttman_errors.RData")

# print
results_guttman_errors %>%
  round_df(3) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

## Rates by criterion and site

```{r}

# data_processed %>%
#   select(-DV) %>%
#   group_by(data_collection_site) %>%
#   summarize_all(.funs = mean) %>%
#   round_df(3) %>%
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# means and sds of exclusion rates across sites
exclusion_rate_by_site <- data_processed %>%
  dplyr::select(-DV, -condition, -DV_uninverted) %>%
  group_by(data_collection_site) %>%
  summarize_all(.funs = mean) %>%
  ungroup()

exclusion_rate_by_site %>%
  round_df(3) %>%
  gather(criterion, proportion, c(exclude_aware_olsen_and_fazio,
                                  exclude_aware_olsen_and_fazio_modified,
                                  exclude_awareness_baranan_dehouwer_nosek,
                                  exclude_awareness_baranan_dehouwer_nosek_modified,
                                  exclude_all_four_combined)) %>% 
  group_by(criterion) %>%
  dplyr::summarize(min_prop = min(proportion),
                   max_prop = max(proportion)) %>%
  round_df(3) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

## Meta analyses of awareness rates

```{r}

n_by_site <- data_processed %>%
  group_by(data_collection_site) %>%
  dplyr::summarize(ni = n())

exclusion_n_by_site <- data_processed %>%
  dplyr::select(-DV, -condition, -DV_uninverted) %>%
  group_by(data_collection_site) %>%
  mutate_at(vars(-group_cols()), as.numeric) %>%
  summarize_all(.funs = sum) %>%
  ungroup() %>%
  left_join(n_by_site, by = "data_collection_site")

```

### Olson & Fazio criterion

```{r}

fit_awareness_rate_of <- exclusion_n_by_site %>%
  rename(criterion = exclude_aware_olsen_and_fazio) %>%
  meta_analysis_proportions()

#summary(fit_awareness_rate_of)

forest(fit_awareness_rate_of, 
       transf = transf.ilogit,
       mlab = add_heterogeneity_metrics_to_forest(fit_awareness_rate_of),
       xlim = c(-1, 1.5),
       at = c(0, .2, .4, .6, .8, 1),
       addcred = TRUE,
       refline = NULL,
       xlab = "Proportion of aware participants")
text(-1, 14, "Olson & Fazio (2001) exclusions", pos = 4)
text(1.5, 13.85, "Proportion [95% CI]", pos = 2)

```

### Olson & Fazio modified criterion

```{r}

fit_awareness_rate_ofmod <- exclusion_n_by_site %>%
  rename(criterion = exclude_aware_olsen_and_fazio_modified) %>%
  meta_analysis_proportions()

#summary(fit_awareness_rate_ofmod)

forest(fit_awareness_rate_ofmod, 
       transf = transf.ilogit,
       mlab = add_heterogeneity_metrics_to_forest(fit_awareness_rate_ofmod),
       xlim = c(-1, 1.5),
       at = c(0, .2, .4, .6, .8, 1),
       addcred = TRUE,
       refline = NULL,
       xlab = "Proportion of aware participants")
text(-1, 14, "Olson & Fazio (2001) modified exclusions", pos = 4)
text(1.5, 13.85, "Proportion [95% CI]", pos = 2)

```

### Bar-Anan et al criterion

```{r}

fit_awareness_rate_bdn <- exclusion_n_by_site %>%
  rename(criterion = exclude_awareness_baranan_dehouwer_nosek) %>%
  meta_analysis_proportions()

#summary(fit_awareness_rate_bdn)

forest(fit_awareness_rate_bdn, 
       transf = transf.ilogit,
       mlab = add_heterogeneity_metrics_to_forest(fit_awareness_rate_bdn),
       xlim = c(-1, 1.5),
       at = c(0, .2, .4, .6, .8, 1),
       addcred = TRUE,
       refline = NULL,
       xlab = "Proportion of aware participants")
text(-1, 14, "Bar-Anan et al. (2010) exclusions", pos = 4)
text(1.5, 13.85, "Proportion [95% CI]", pos = 2)

```

### Bar-Anan et al modified criterion

```{r}

fit_awareness_rate_bdnmod <- exclusion_n_by_site %>%
  rename(criterion = exclude_awareness_baranan_dehouwer_nosek_modified) %>%
  meta_analysis_proportions()

#summary(fit_awareness_rate_bdnmod)

forest(fit_awareness_rate_bdnmod, 
       transf = transf.ilogit,
       mlab = add_heterogeneity_metrics_to_forest(fit_awareness_rate_bdnmod),
       xlim = c(-1, 1.5),
       at = c(0, .2, .4, .6, .8, 1),
       addcred = TRUE,
       refline = NULL,
       xlab = "Proportion of aware participants")
text(-1, 14, "Bar-Anan et al. (2010) modified exclusions", pos = 4)
text(1.5, 13.85, "Proportion [95% CI]", pos = 2)

```

### Summary

For each awareness criterion, the proportion of participants at each site that were labelled as demonstrating awareness was calculated and subjected to a meta analysis of proprortion. Results suggested that the variation in exclusion rates between sites represented a large degree of between-site heterogeneity rather than merely sampling variation (across exclusion criteria, all I2 = 54.7% to 91.7%, all H2 = 2.2 to 12). This may suggest that the awareness exclusion criteria were not functioning equivalently as measures of awareness between sites. This seems plausible given that participants' open ended responses were hand scored by researchers, making scores far from objective. 

# New meta analysis: EC effect using compound exclusion criterion

We provide a more severe test of the verbal hypothesis: participants are scored as aware if *any* of the four criteria flag them as aware and excluded from the meta analysis.

```{r}

n_participants <- data_combined_criteria %>%
  select(DV, data_collection_site) %>%
  count() %>%
  pull(n)

k_sites <- data_combined_criteria %>%
  distinct(data_collection_site) %>%
  count() %>%
  pull(n)

```

N after combined exclusions = `r n_participants`, k sites = `r k_sites`.

## Power analyses

Before conducting such an analysis, it is useful to first consider what power such an analysis would have given an increased exclusion rate. 

### Method reported in manuscript 

For the sake of a like-for-like comparison, it is useful to report power calculations using the same method as reported in Moral et al.

From the manuscript, this was a power analysis for a (fixed effects model) Cohen's *d*: "within subjects [t test], one-tailed, alpha = 0.05)"

```{r eval=FALSE, include=TRUE}

# meta effect size from published literature
pwr.t.test(d = 0.20,   # CHANGED
           n = n_participants, 
           sig.level = 0.05, 
           type = "paired", 
           alternative = "greater")
# >power = 0.9997737

# min es with power >= .99
pwr.t.test(d = 0.16,   # CHANGED
           n = n_participants, 
           sig.level = 0.05, 
           type = "paired", 
           alternative = "greater")
# >power = 0.9933745

# min es with power >= .80
pwr.t.test(d = 0.10,    # CHANGED
           n = n_participants,
           sig.level = 0.05, 
           type = "paired", 
           alternative = "greater")
# >power = 0.8241449

```

- At *d* = .20, the meta effect size from published literature, >99% power to detect this effect
- At 99% power criterion, detectable *d* = .16 
- At 80% power criterion, detectable *d* = .10

While I report this method for the sake of parity with the method reported in Moran et al., I disagree with several aspects of this analysis. First, statistical power is a property of a given hypothesis test method, and the test used in both Moran et al and here is meta analytic effect size (i.e., which involves a random effect for site). This power analysis does not involve this random effect - it is a power analysis for a different test, and it is unclear the degree to which it is informative for the hypothesis test that was actually run, both in Moran et al. and here. Even putting this aside, other issues are also apparently present: 1) the power analysis employed in Moran et al was stated to be a within subjects effect size, but the study design actually used a one-sample test; 2) the power analysis was stated to be one-sided with alpha = 0.05, but the hypothesis test uses two-sided with alpha = 0.05 (i.e., equivalent to one-sided with alpha = 0.10). In light of these issues, I also conducted what appeared to be a more appropriate power analysis below.

### For multi-level model

This analysis acknowledges the hierarchical structure among the data given the multi-site design. 

Heterogeneity ($\tau^{2}$) set to 0 given (a) all sites used standardized materials (a priori) and (b) results from preregistered meta analyses using other exclusion criteria demonstrated no heterogeneity (somewhat post hoc, but still useful to know). $\alpha$ = 0.05, two-sided.

```{r eval=FALSE, include=TRUE}

# each of the below returns power estimate. yi was tuned for each to find criterion power values. yi values then reported below.

# meta effect size from published literature
power_meta(yi = 0.2, 
           ni = n_participants/k_sites,
           k  = k_sites,
           tau2 = 0.0)

# 99% power 
power_meta(yi = 0.24, # criterion effect size
           ni = n_participants/k_sites,  # average n per site
           k  = k_sites,  # k sites
           tau2 = 0.0)  # heterogeneity

# 80% power 
power_meta(yi = 0.16, 
           ni = n_participants/k_sites,
           k  = k_sites,
           tau2 = 0.0)

```

- At *d* = .20, the meta effect size from published literature, 95% power to detect this effect
- At 99% power criterion, detectable meta *d* = .24
- At 80% power criterion, detectable meta *d* = .16

## Frequentist meta-analysis

```{r}

results_combined_criteria <- meta_analysis_workflow(data_combined_criteria)

```

- Meta anaysis results: `r results_combined_criteria$meta_analysis_results_text`
- Heterogeneity tests: `r results_combined_criteria$heterogeneity_test_results_text`

## Bayes factor meta-analysis

Using Rouder and Morey (2011) method: First calculate t values for each site, then use the "meta.ttestBF" function to calculate a meta analysis BF.

```{r}

results_ttests <- data_combined_criteria %>% 
  select(data_collection_site, DV) %>%
  group_by(data_collection_site) %>%
  do(ttest = tidy(t.test(.$DV))) %>%
  unnest(ttest) %>%
  rename(t = statistic, 
         df = parameter) %>%
  mutate(n = df + 1)

meta_bf <-
  meta.ttestBF(t = results_ttests$t,
             n1 = results_ttests$n,
             rscale = sqrt(2)/2)

meta_bf_samples <-
  meta.ttestBF(t = results_ttests$t,
               n1 = results_ttests$n,
               rscale = sqrt(2)/2,
               posterior = TRUE,
               iterations = 10000)

bf10 <- data.frame(key = "BF10", value = extractBF(meta_bf)$bf)

meta_bf_samples %>%
  as.data.frame() %>%
  dplyr::summarize(delta_estimate = quantile(delta, 0.500),
                   hdi_lower      = quantile(delta, 0.025),
                   hdi_upper      = quantile(delta, 0.975)) %>%
  gather() %>%
  bind_rows(bf10) %>%
  round_df(2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

